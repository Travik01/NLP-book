
  ПОСОБИЕ ПО NLP
 
Оглавление
Список опечаток…………………………………………………………………………
Об авторе обложки………………………………………………………………………
Об авторе книги………………………………………………………………………….
Вводная часть………………………………………………………………..……….….
Глава 1 введение……………….……………………………………………………….
1.1	 Нейронные сети и глубокое обучение……………………….……….
1.2	 NLP в глубоком обучении……………………………………….…………
Глава 2 библиотеки……………………………………………………………………
  2.1  Os, json……………………………………………………………………………..
  2.2  SpeechRecongnition, pyttsx3……………………………………………….
  2.3  Matplotlib………………………………………………………………………….
  2.4  Numpy, pandas…………………………………………………………………..
  2.5  NLTK………………………………………………………………………………..
  2.6  Sklearn, tensorflow,Pytorch…………………………………………………
Глава 3 Датасет, обработка и представление………………………………
  3.1 Сбор данных……………………………………………………………………..
  3.2  Обработка данных…………………………………………………………….
  3.3  Удаление “мусора”…………………………………………………………….
  3.4  Токинизация…………………………………………………………………….
    3.4.1 Токинизация по предложениям………………………………………
    3.4.2 Токинизация по словам………………………………………………….
  3.5  Лематизация……………………………………………………………..………
  3.6 Мешок слов……….……………………………………………………………....
  3.7  Стоп слова…………………………………………………………………………
  3.8  Регулярные выражения……………………………………………………..
Глава 4 Основы машинного и глубокого обучения……………………….
  4.1 Обучение с учителем…………………………………………………………..
    4.1.1 Регрессия как метод………………………………………………………
    4.1.2 Классификация как метод………………………………………………
  4.2 Обучение без учителя………………………………………………………….
    4.2.1 Ассациация……………………………………………………………………
    4.2.2 Сокращение…………………………………………………………………..
Глава 5 NLP……………………………………………………………………………….
  5.1 RNN…………………………………………………………………………………..
    5.1.1 LSTM………………………………………………………………………….…
    5.1.2 GRU………………………………………………………………………………
    5.1.3 Tree Lstm……………………………………………………………………
  5.2 Transformers……………………………………………………………………
  5.3 Машинное обучение………………………………………………………..
Глава 6 Практические задачи …………………………………………………
Глава 7 Итог…………………………………………………………………………..
  7.1 NLP не один такой………….….……………………………………………
  7.2 Интерфейс………………………………………………………………………
  7.3 Что дальше?.................................
  7.4 Результаты……………………………………………………………………..
Список литературы…………………………………………………………………

  

  
	

   





 
Список опечаток 

Хотя мы приняли все возможные меры для того, чтобы удостовериться в качестве наших текстов, ошибки все равно случаются. Если вы найдете ошибку в тексте или в коде, мы будем очень благодарны, если вы сообщите нам о ней. Сделав это, вы избавите других читателей от
расстройств и поможете нам улучшить последующие версии этого пособия. Если вы найдете какие-либо ошибки, пожалуйста, сообщите о них нам написав на почту  masasaposnik24@gmail.com
 

Об авторе обложки 
Автор обложки - художник kvolixx. Просим не использовать данное изображение в различных источниках без указания автора картины или названия пособия, во избежание проблем с авторскими правами.

Об авторе книги
Автор книги обычная ученица 9 класса. В этой книге я старалась разжевать то, что разжёвывала около 2-х лет, пока изучала различного рода обучения машин. Как только я получу более профессиональные знания в этой сфере, я обязательно расскажу о них на “легком” языке в следующих пособиях.
 
      ВВОДНАЯ ЧАСТЬ
Обработка естественного языка (NLP - Natural Language Processing) представляет собой область, связанную с различными методами компьютерной обработки человеческой речи. Это включает как алгоритмы, анализирующие тексты, созданные людьми, так и алгоритмы, способные генерировать тексты, которые выглядят максимально естественно. Спрос на такие технологии неуклонно растет: разработчики постоянно совершенствуют код, увеличивают объемы данных для анализа и ожидают, что компьютеры смогут взаимодействовать с пользователями на их родном языке. Однако задачи, связанные с обработкой естественного языка, остаются крайне сложными из-за присущей языкам неоднозначности, изменчивости и постоянного развития. Слова часто имеют множественные значения, а их интерпретация зависит от контекста. 

Изначально естественный язык рассматривался как символьная система, и первые попытки его обработки основывались на логике и правилах. Однако из-за высокой степени неоднозначности и постоянного расширения словарного запаса потребовались более гибкие подходы, основанные на статистике. В последние десятилетия наиболее популярными стали методы машинного обучения. Более десяти лет в NLP доминировали линейные модели с учителем, такие как перцептроны, метод опорных векторов и логистическая регрессия. Эти модели работали с высокоразмерными, но разреженными векторами данных. 
Примерно с 2014 года в отрасли начался переход от линейных моделей к нелинейным нейросетевым архитектурам, работающим с плотными представлениями данных. Некоторые нейросетевые методы являются обобщением линейных моделей и могут их заменять в задачах классификации. Другие, более сложные подходы, требуют пересмотра постановки задачи, но открывают новые возможности для моделирования. Например, рекуррентные нейронные сети (РНС) используют марковское предположение, характерное для последовательностных моделей, и позволяют учитывать длинные последовательности данных, что делает их мощными инструментами для извлечения признаков. 

Эти достижения привели к значительным прорывам в таких областях, как языковое моделирование, автоматический перевод и других приложениях NLP. Несмотря на свою эффективность, нейросетевые методы требуют глубоких знаний и опыта, что создает высокий барьер для входа в область. В этой книге я постаралась объединить информацию, которая будет полезна как опытным специалистам, так и новичкам. Цель — познакомить читателей с основными концепциями, терминологией, инструментами и методами, необходимыми для понимания и применения нейросетевых моделей в обработке естественного языка.

Общее описание бы
Книга задумана самостоятельной, представляющей различные подходы , в  единой системе иметодики. Однако основное ее назначение - быть инструктором в  введении  в механизм нейронных сетей (глубокого обучения) и их применение к лингвистическим данным, а не углубленное изложение основ теории машинного обучения и технологии обработки естественного языка. Аналогично книга не является исчерпывающим источником для тех, кто намерен развивать нейросетевые подходы (хотя и может служить неплохой отправной точкой). На момент написания этой книги исследования в области нейронных сетей и глубокого обучения идут полным ходом.(DeepsSeek, chatGPT-4 и т.д). Поэтому моя задача - охватить  методы, доказавшие свою работоспособность в нескольких приложениях, а также несколько библеотек языка
 

     ГЛАВА 1 
Введение : проблемы естественного языка

Обработка естественного языка - это наука о проектировании методов и алгоритмов, которые принимают или порождают неструктурированные данные естественного языка. Человеческие языки в высшей степени неоднозначны (сравните фразы «Я ел суши с друзьями» и «Я ел суши с огурцом") и вариативны (смысл фразы «Я ел суши с друзьями» можно выразить и так: «Мы с друзьями разделили суши»). Языки также постоянно изменяются и развиваются. Люди прекрасно справляются с порождением и пониманием языковых конструкций, они способны выразить, воспринять и интерпретировать фразы с утонченным смыслом. Но, будучи замечательными пользователями языка, люди очень плохо справляются с формальным описанием правил, управляющих языком. Поэтому понимание и порождение языка с помощью компьютеров - чрезвычайно трудная задача. Лучшие методы работы с языковыми данными основаны на алгоритмах машинного обучения с учителем, которые пытаются вывести паттерны и закономерности использования из множества предварительно аннотированных пар входных и выходных текстов. 
Помимо проблем, связанных с обработкой неоднозначных и вариативных входных данных в системе с плохо определенными и отсутствующими наборами правил, у естественного языка есть и дополнительные свойства, которые еще больше затрудняют разработку вычислительных подходов на основе машинного
обучения: дискретность, композиционность и разреженность.
Язык по природе своей символический и дискретный. Основными элементами письменного языка являются литеры. Литеры образуют слова, обозначающие предметы, понятия, события, действия и идеи. Литеры и слова - дискретные символы: слова «гамбургер» или «пицца» вызывают определенные мысленные образы, но также являются разными символами, смысл которых отделен от них и интерпретируется в нашем мозгу. Не существует внутренней связи между «гамбургером» и «пиццей», которую можно было бы вывести из самих символов или составляющих их букв. Сравните это с таким понятием, как цвет, играющий важнейшую роль в машинном зрении, или звуковые сигналы: эти величины непрерывны, что позволяет, например, перейти от цветного изображения к тоновому с помощью простой математической операции, или сравнить два цвета на основе внутренне присущих им свойств, например оттенка и яркости. Со словами так не получится - не существует простой операции, которая позволила бы перейти от слова «красный» к слову «розовый» без использования большой справочной
таблицы или словаря. Язык также обладает свойством композиционности: буквы образуют слова, слова образуют фразы и предложения. Смысл фразы может быть больше смысла составляющих ее слов и определяется набором запутанных правил. Поэтому, чтобы интерпретировать текст, приходится подняться над уровнем букв и слов и рассматривать длинные последовательности слов, например предложения или
даже полные документы. Сочетание описанных выше свойств ведет к разреженности данных. Число ком-
бинаций слов (дискретных символов), имеющих смысл, практически бесконечно. Число допустимых предложений огромно, нет никакой надежды перечислить их все. Откройте любую книгу - подавляющее большинство встречающихся в ней предложений вы никогда раньше не видели и не слышали. Более того, вполне веро- ятно, что многие последовательности четырех слов, встречающиеся в этой книге, тоже не попадались вам ранее. Если вы возьмете газету, вышедшую каких-то 10 лет назад, или попробуете вообразить газету, которая выйдет через 10 лет, то многие слова, особенно имена людей, названия торговых марок и компаний, а также жаргонные словечки и технические термины, покажутся незнакомыми. Нет никакого очевидного способа получить одно предложение из другого путем обобщения или определить сходство предложений, не зависящее от их смысла, который для нас является ненаблюдаемой величиной. Поэтому задача обучения на примерах становится крайне трудной: даже располагая гигантским набором примеров, мы все равно с большой вероятностью будем наблюдать события, которые в этом наборе
не встречались и сильно отличаются от всех встречающихся примеров



1.1 Нейронные  сети и глубокое обучение
 
С технической точки зрения глубокое обучение это многоступенчатый способ получения представления данных. Идея проста, но, оказывается, очень простые механизмы в определенном масштабе могут выглядеть непонятными и таинственными. Под «глубиной» в глубоком обучении не подразумевается более детальное понимание, достигаемое этим подходом; идея заключается в создании многослойного представления. Поэтому подходящими названиями для этой области машинного обучения могли бы также служить многослойное обучение. Число слоев, на которые делится модель данных, называют глубиной модели. 
Современное глубокое обучение часто вовлекает в процесс десятки и даже сотни последовательных слоев представления все они автоматически определяются на основе обучающих данных. Тогда как другие подходы машинного обучения ориентированы на изучение всего одного-двух слоев. В глубоком обучении такие многослойные представления рассматриваются (почти всегда) с использованием нейронных сетей моделей, структурированных в виде слоев, наложенных друг на друга. Термин «нейронная сеть» пришел к нам из нейробиологии; хотя некоторые основополагающие идеи глубокого обуче-ния действительно отчасти заимствованы из науки о мозге, все же нету  никаких доказательств, что мозг реализует механизмы, подобные используемым в глубоком обучении. 
Как выглядят представления, получаемые алгоритмом глубокого обучения?

 
Рис 1.1

Давайте на примере данных mnist исследуем, как сеть, имеющая несколько слоев , преобразует изображение цифры, пытаясь ее распознать. Как показано на рис. 1.2, сеть поэтапно преобразует образ цифры в представление, все больше отличающееся от исходного изображения и несущее все больше полезной информации для нее .Глубокую сеть можно представить как многоэтапную операцию очистки, во время которой информация проходит через последовательность фильтров и выходит из нее в очищенном виде.
Если хотите рассмотреть фильтрацию по подробней можете посетить сайт https://adamharley.com/nn_vis/cnn/3d.html


1.2 NLP в глубоком обучении

Глубокое обучение в обработке естественного языка (NLP) использует нейронные сети для анализа и генерации текста. Процесс начинается с токенизации, где текст разбивается на отдельные слова или подслова, а затем происходит векторизация, преобразующая токены в числовые представления с помощью методов, таких как one-hot encoding(это метод преобразования категориальных переменных в двоичный формат)или word embeddings(это способ представления слов или фраз в виде числовых векторов). Основные архитектуры нейронных сетей включают рекуррентные нейронные сети (RNN), их улучшенные версии — LSTM и GRU(подробнее можете узнать тут https://habr.com/ru/companies/mvideo/articles/780774/) , а также трансформеры, которые используют механизм внимания для обработки текста. Модели обучаются на больших объемах неразмеченных данных (предобучение) и адаптируются к конкретным задачам на меньших размеченных наборах (дообучение). Механизм внимания позволяет модели фокусироваться на значимых частях текста, что важно для понимания контекста. После обучения модели могут выполнять различные задачи, такие как перевод или анализ тональности, с использованием методов генерации, включая greedy search (это подход к решению проблемы, при котором на каждом шаге выбирается лучший вариант из доступных на данный момент , наглядный пример это в пункте 1.2 описание работы на надоьре данных mnist) и sampling(это процесс формирования выборки данных, которая в достаточной мере отражает свойства всей исследуемой совокупности). Однако использование глубокого обучения в NLP также поднимает вопросы о предвзятости данных и безопасности, что требует внимательного подхода к этическим аспектам.
Если вам кажется все еще какой то мастикой или невероятной загадкой , ответ до которой придеться осмысливать несколько лет , то не беспокойтесь. Скорее вас пугает изобилие новых и сложных слов, поэтому давайте познакомимся со всем поближе на практике , уверяю, вы подружитесь.





 
ЧАСТЬ 1
ГЛАВА 2 библеотеки 

Перед началом давайте договоримся , что при построении сети у нас будет примерно такой поверхностный план 
0 построение задачи 
1 выбор и обработка датасета 
2 построение модели
3 разделение данных
4 обучение на данных
5 проверка аккуратности(включая графики) и корректировка  значений для лучшения точности 
И так , для выполнения различных действия из плана нам нужны функции которые будут заменять нам ручные вычисления  и настройки.Как вы знаете язык Python уже содержит основные функции , но для узкоспециальных заданий как наши их просто не хватит , поэтому мы будем использовать различные библиотеки.В этой главе мы рассмотрим основные, которые будут нам встречаться чаще всего, но по мере прохождения тем мы еще познакомимся с несколькими библиотеками. 

На случай если вы не знали или забыли как скачивать не встроенные библеотеки то прочитайте данная статья будет вам полезна https://help.reg.ru/support/servery-vps/oblachnyye-servery/ustanovka-programmnogo-obespecheniya/kak-ustanovit-biblioteku-v-python#0



2.1 os,json

os- встроенная библиотека для работы с файлами.Часто на практике нужно будет изменения компоненты проекта (добавлять , удалять файлы и т.д).Данная библеотеки хорошо подходит, так как благодаря ее функциям можно будет через код работать с файлами
Основные функции которые могут понадобятся при работе:
os.makedirs(). Создаёт новую директорию по указанному пути. Если указанный путь не существует, то функция создаст его. 
os.rename(). Используется для переименования файла или директории. Принимает два аргумента: путь к переименовываемому файлу или директории и новое имя файла или директории вместе с полным путём. 
os.remove(). Удаляет указанный файл. Чтобы избежать ошибки, указанный файл должен существовать.
os.getcwd(). Позволяет узнать текущую рабочую директорию, в которой осуществляется выполнение программы.
Хочу отметить , что часто при скачивании датасета, вы получите данные в формате zip.На сколько мы знаем, количество данных порой бывает колоссальных размеров, поэтому скачиваемые данные в таком формате. Благодаря os можно распаковать зип папку,а ее содержимое разложить по нужным местам в проекте прямо через код.
И помните , правильное структурирование файлов - залог успеха! 

JSON -библеотека для работы с json файлами. Впервые слышите о формате json? Этот формат будет играть не маленькую роль в создании нейронных сетей, так как в нем можно кодировать и декодировать информацию. Например, с его помощью можно представлять память человека в виде JSON-словаря и после каждого ответа ИИ в диалоге извлекать важную информацию из текущего диалога и складывать её в соответствующий раздел словаря.Так же иногда в формате json хранят уже готовые нейронные сети 
Основные функции которые могут нам понадобиться: 
json.dump(). Записывает Python-объекты в файл JSON. 
json.dumps(). Сохраняет объект в виде строки с нотацией JSON. 
json.load(). Прочитывает файл JSON и возвращает его в виде объекта. 
json.loads(). Преобразует строку с нотацией JSON в объект.

2.2 SpeechRecognition,pyttsx3

У меня идет прямая связь текстовых нейросетей и голосовых помощников, поэтому не могу не познакомить с еще одной важной библеотекой содержащий несколько пред обученных моделей это SpeechRecognition.Библиотека  SpeechRecognition — мощный и простой инструмент для интеграции голосового управления в проекты на Python
Работает все интуитивно понятно, ниже пример рабочего кода:

import speech_recognition as sr
# Создаём объект распознавателя
recognizer = sr.Recognizer()
# Используем микрофон для записи
with sr.Microphone() as source:
    print("Скажите что-нибудь:")
  recognizer.adjust_for_ambient_noise(source)  # Настройка на шум
    audio = recognizer.listen(source, timeout=5)  # Запись звука
try:
   # Распознавание речи через Google API
     text = recognizer.recognize_google(audio, language="ru-RU")
    print(f"Вы сказали: {text}")
except sr.UnknownValueError:
    print("Не удалось распознать речь.")
except sr.RequestError as e:
    print(f"Ошибка сервиса: {e}")
В примере используется пред обученная модель от google. Всего три пред обученные модели на выбор в SpeechRecognition это 

Ассистент конечно должен ответить пользователю с этим нам поможет pyttsx3. pyttsx3 - библеоткка которая преобразовывает текст в речь, она поддерживает множество различных  языков, а так же голосов( к сожалению не на всех устройствах поддерживаются различные голоса, а так же основной(системный) голос будет зависеть от операционной системы)
Так же по виду для наглядности пример кода:
import pyttsx3
# Инициализация движка
engine = pyttsx3.init()
# Текст для озвучивания
text = "Привет! Добро пожаловать в мир Python."
# Озвучивание текста
engine.say(text)
# Завершение работы движка
engine.runAndWait()

2.3 Matplotlib
Matplotlib - библиотека  о которой первое впечатление это «как не сломать язык?».Давайте ближе к сути Matplotlib - библеотека которая используется для создание двухмерных и трехмерных графиков.Вы явно зададитесь вопросом, зачем же нам графики ?
Ответ прост, зависимости. Зависимости- интересное слово, в нашем случае зависимости датасета.Чтобы  понять как писать нейросети, надо понять как они думают. Для этого в данных (чаще всего с помощью тепловой карты) рассматривается зависимость одних данных от других. Например нейросети нужно определить одобрят ли человеку кредит из ходя  из его критериев. Мы точно можем сказать что имя не зависит, а зарплата играет одну из ключевых ролей, но не все критерии могут быть такими очевидными( в будущем вообще не  влияющие критерии можно удалить тем самым не перегружать нейросеть и не занимать лишнюю память, а ключевым уделить побольше внимания и  последующие рассмотрение) 


2.4 numpy, pandas
Как мы знаем качество нейросети на прямую зависит от данных и порой данные нужно менять (если берем уже готовый датасет и форма данных требует изменению по вашему мнению). С работой данных нам поможет numpy numpy основные Некоторые основные функции библиотеки NumPy:

np.zeros((n, m)). Создаёт массив размером n*m, заполненный нулями. После кортежа с размерностью массива можно указать тип элементов в массиве (по умолчанию — float64). 
np.ones((n, m)). Создаёт массив размером n*m, заполненный единицами. 
min() и max(). Возвращают минимальное и максимальное значение из элементов массива. 
argmax(). Возвращает индекс максимального значения из массива. Поечму функция выделенна? Все просто, мы будем пользоваться этой функцией часто, так как нейросети выдают ответ в виде массива , который состоит из вероятностей ( нейросеть на каждый обьект будет выдавать процент- на сколько она думет, что это ответ)

2.5 NLTK
Natural Language Toolkit (NLTK) — это популярная библиотека с открытым исходным кодом для обработки естественного языка. Как уже стало понятно, она помжет при обработке данных. Данные можно обрабатывать и представлять различными способами, поэтому функции этой библиотеки рассмотрим в следующей главе.

2.6 sklearn, tensorflow,PyTorch
И так, мы подошли к самым интересным библеотекам, конечно по моему мнению, но надеюсь после прочтения будет "по нашему мнению". Scikit-learn (sklearn) — это один из наиболее широко используемых пакетов Python для Data Science и Machine Learning. Он содержит функции и алгоритмы для машинного обучения: классификации, прогнозирования или разбивки данных на группы.(выделить)
Как вы поняли sklearn - первое что приходит в голову специалистом по машинному обучению когда дело касается кода, а специалистам по глубокому? - tensorflow и PyTorch! TensorFlow — открытая программная библиотека для машинного обучения, разработанная компанией Google. Она позволяет решать задачи построения и тренировки нейронной сети для автоматического нахождения и классификации образов. Библиотека даёт возможность создавать сложные нейронные сети.
PyTorch -  библиотека для искусственного интеллекта и нейросетей. Позволяет строить классические архитектуры нейросетей с помощью готовых блоков, а при необходимости решать более низкоуровневые задачи, например оптимизировать вычисления на графическом процессоре GPU
GPU-графический процессор, как мы все знаем размеры нейросетей могут быть очень большими и если вы хотите запустить их на своем устройстве еще и добиться быстрых ответов то GPU вам в помощь.Как вы уже поняли , вычисления будут происходить не на вашем процессоре, а на графическом. Чаще всего мы будем использовать cuda, как его установить вы можете узнать у них на официальном сайте https://developer.nvidia.com/how-to-cuda-python
 

Глава 3 Датасет, обработка и представление 
3.1 Сбор данных
Для сбора данных дата-сайентисты используют два подхода: либо собирают их из открытых источников, например из социальных сетей, либо пользуются информацией, собранной компанией. Например, крупные онлайн-магазины могут обучать модели на истории заказов своих клиентов. Это становится возможным благодаря их объёму — информации о миллионах и десятках миллионов покупок.
Данные из открытых источников за счёт своего разнообразия помогают построить универсальные языковые модели. ChatGPT был обучен на большом массиве открытых данных, поэтому он одинаково хорошо генерирует как сказки, так и юридические документы.
Модель NLP можно собрать на данных одного человека. Например, сделать чат-бота, который будет имитировать речь и манеру общения своего живого прототипа. Инфлюенсер из США Карин Марджори сделала свою копию, которая может стать виртуальной девушкой для любого желающего — всего за один доллар в минуту. Виртуальная копия Карин обучалась на видео с её ютуб-канала.( иностранный владелец Youtube нарушает закон РФ).Так же хочется отметить данные для обучения созданные уже готовыми языковыми моделями, однако, такие данные применяются редко  так как данные выданные нейросетью не всегда точны.

3.2 Обработка данных 
Далее, что бы преподнести эти данные нашей нейросети для обучения их надо обработать.

3.3 Удаление “мусора”
Как мы уже упоминали, в составе изначальных данных могут находить “мусор” – данные которые не имеют значение или же, что часто в NLP дубликаты. Чем больше они загрязнены, тем сложнее модели будет понять, что важно, а что нет. Поэтому специалисты предварительно удаляют повторы, приводят строки к одному регистру и удаляют ненужные символы.

3.4 Токинизация
Страшное слово, которого даже я по началу боялась, это стало первым страшным словом для меня в этой сфере, но на деле это слово оказалось очень даже притяным, давайте познакомлю вас. Токинизация (иногда – сегментация) по предложениям – это процесс разделения письменного языка на предложения-компоненты. Идея выглядит довольно простой. В английском и некоторых других языках мы можем вычленять предложение каждый раз, когда находим определенный знак пунктуации – точку.
Токены могут быть словами, символами, фразами или другими элементами, в зависимости от задачи и контекста. Предлагаю рассмотреть несколько часто используемых вариантов

3.4.1 Токинезация по предложениям
Чтобы сделать токенизацию предложений с помощью NLTK, можно воспользоваться методом nltk.sent_tokenize
В коде это будет выглядеть примерно так (текст в коде сгенерирован случайно так, что не несет никакого смысла):

text = "The constant growth of socio-economic activity allows the sphere of doubt. The importance of their country in the mixed is not fixed by the increasing quantitative level. The third fixes and doubts are increasing, mixed by gradual unrecognizability, but also the developing world is actively developing. Quantitative captures moral worthlessness. captures the perfect development of the brain. The constant activity that is mixed with and the society that status. matters.sentences = nltk.sent_tokenize(text) 
for sentence in sentences:     	
print(sentence)     
print()

На выходе мы получим 5 отдельных предложений

3.4.2 Токинизация по словам 
Давайте возьмем предложения из предыдущего примера(возьмите переменную text из примера сверху) и применим к ним метод nltk.word_tokenize

for sentence in sentences:     
words = nltk.word_tokenize(sentence)     
print(words)     
print()
    
На выходе мы получаем списки со словами, где каждый список это отдельное предложение, но лучше всего запустите код и посмотрите сами, что вам выдаст программа.

3.5 Лемматизация 
Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова. Лемматизация и стемминг преследуют цель привести все встречающиеся словоформы к одной, нормальной словарной форме.
Она может применятся как к словам так и сразу к целым предложениям и выглядит это примерно так :
Исходное слово ---> Корневое слово ---> Характеристика
Встреча ---> Встречаться ---> (извлечение основного слова)
Был ---> Быть ---> (преобразование времени в настоящее)
Мыши ---> Мышь ---> (множественное число в единственное)
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
text = "My cats were playing in the garden"
lemmatized_text = [lemmatizer.lemmatize(word) for word in text.split()]
print(lemmatized_text)
На выходе мы получим: ['My', 'cat', 'were', 'playing', 'in', 'the', 'garden']



3.6 Мешок слов
Мешок слов — это упрощённое представление текста, которое показывает, какие слова встретились в тексте, но при этом не 
учитывает их порядок. Эта модель пытается понять, встречается ли знакомое слово в документе, но не знает, где именно оно встречается.

3.7 Стоп слова 
Стоп слова – это всякие не приятные выражения, которым лучше не учить нашу нейросеть, именно благодаря им, даже при ссоре с искусственным интелектом, он будет дображелательный.
В NLTK есть предустановленный список стоп-слов. Перед первым использованием вам понадобится его скачать: nltk.download(“stopwords”). После скачивания можно импортировать пакет stopwords и посмотреть на сами слова:
from nltk.corpus import stopwords
print(stopwords.words("english")) 
Что бы убрать такие стоп слова из предложения, то воспользуйтесь этим кодом :
stop_words = set(stopwords.words("english")) 
sentence = "Backgammon is one of the oldest known board games." 	  	
words = nltk.word_tokenize(sentence) 	
without_stop_words = [word for word in words if not word in stop_words] 	
print(without_stop_words)
В итогу мы получим список из слов предложения без стоп слов
	
3.8 Регулярные выражения
Регулярное выражение (регулярка, regexp, regex) – это последовательность символов, которая определяет шаблон поиска. Вот несколько примеров с которыми вы можете столкнуться:
. – любой символ, кроме перевода строки;
\w – один символ;
\d – одна цифра;
\s – один пробел;
\W – один НЕсимвол;
\D – одна НЕцифра;
\S – один НЕпробел;
[abc] – находит любой из указанных символов match any of a, b, or c;
[^abc] – находит любой символ, кроме указанных;
[a-g] – находит символ в промежутке от a до g.

Мы можем использовать регулярки для дополнительного фильтрования нашего текста. Например, можно убрать все символы, которые не являются словами. Во многих случаях пунктуация не нужна и ее легко убрать с помощью регулярок.
Модуль re в Python представляет операции с регулярными выражениями. Мы можем использовать функцию re.sub, чтобы заменить все, что подходит под шаблон поиска, на указанную строку. Вот так можно заменить все НЕслова на пробелы:
import re
sentence = "The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."
pattern = r"[^\w]"
print(re.sub(pattern, " ", sentence))


	



















ГЛАВА 4 основы машинного и глубокого обучение, методы решения задач 

В этой главе приводится краткое введение в терминологию и практику машинного обучения. Здесь же подготавливается почва и определяется нотация, используемая в последующих главах.. Теория машинного обучения с учителем и линейные модели - очень обширные темы, так что нельзя считать, что глава охватит всю тему целиком 


4.1 обучение с учителем 

Метод обучения с учителем (supervised learning) аналогичен получению знаний в школе, где нейросеть выступает в качестве ученика, а человек — в роли преподавателя. Роль учителя заключается в том, чтобы подать на вход модели исходные данные и их «расшифровку». Например, из набора данных с двумя переменными: возраст (входные данные) и рост (выходные данные), можно реализовать модель обучения для прогнозирования роста человека на основе его возраста.
Например, при обучении задачи где нам нужно изходя  из параметров определить психологическую оценку и каждому человеку будет уже присвоена метка (его спих. Оценка). Так происходит настройка параметров для минимизации ошибок между собственными предположениями и «правильными ответами» (метками). Сопоставляя их из раза в раз, нейронная сеть будет самостоятельно обучаться отвечать и на последующие запросы правильно уже без помощи человека.

Тренировочный набор данных для этого типа обучения важно разметить, то есть каждому примеру сопоставить результат, который модель должна получить. Для этого над входным датасетом следует предварительно поработать: учитель собирает его заранее, просматривает и размечает в понятном для обработки виде или же берет уже размечанные  датасеты из открытых источников, зачастую это Keggle(https://www.kaggle.com) или Hugging face(https://huggingface.com).


P.s советую поближе познакомиться с Keggle и Huggle face, так как на данных сайтах разрезано огромное количество ее только данных, но и готовых моделей(включая всеми известных GPT,Qwen,DeepSeek и т.д)
 
Главная проблема такого формата обучения — необходимость сбора и обработки огромных массивов информации на соответствующих высоких мощностях. Это длительный, дорогостоящий и технически сложный процесс, позволить себе который могут только крупные компании, не говоря уже о частных лицах. Кроме того, обучение с учителем подходит далеко не для всех типов данных. Оно предполагает, что в дальнейшем система будет работать только с информацией, аналогичной обучающему датасету, иначе эффективность ее функционирования точно предсказать невозможно.

Обучение с учителем используется для нейросетей, которые в дальнейшем будут решать задачи классификации , регрессии, дерева  решений, случайного леса и еще других  методов, но мы рассмотрим основные.(если хотите подробней ознакомиться со всеми то советую прочитать  https://habr.com/ru/companies/sberbank/articles/840554/) 

4.1.1 Регрессия как метод.
В процессе создания регрессионной модели, все что мы пытаемся сделать — это нарисовать линию настолько близко к каждой точке из набора данных, на сколько это возможно.  Типичный пример такого подхода — «метод наименьших квадратов» линейной регрессии, с помощью которого рассчитывают близость линии в направлении верх-низ. Когда вы создаете регрессионную модель, ваш конечный продукт — это уравнение, с помощью которого вы можете предсказать значения y для значения x без того чтобы знать значение y наперед 
  
Рис 4.1 регрессия и классификация

4.1.2 Классификация как метод 
Классификация — это один из ключевых методов машинного обучения, который используется для предсказания категориальных меток на основе входных данных. В отличие от регрессии, где предсказываются непрерывные значения, классификация фокусируется на предсказании дискретных меток, таких как "да" или "нет", "красный" или "синий", "болен" или "здоров". Классификация находит широкое применение в различных областях, включая медицинскую диагностику, спам-фильтрацию и распознавание изображений. Важно отметить, что классификация может быть бинарной (два класса) или многоклассовой (более двух классов), в зависимости от задачи.
Классификация играет важную роль в современном мире, где огромное количество данных требует автоматической обработки и анализа. Например, в медицинской диагностике классификационные модели могут помочь врачам быстрее и точнее ставить диагнозы, что в конечном итоге спасает жизни. 


4.2 Обучение без учителя 
Кластеризация — это задача группировки множества объектов на подмножества (кластеры) таким образом, чтобы объекты из одного кластера были более похожи друг на друга, чем на объекты из других кластеров по какому-либо критерию. Кластер формируется на основе какой-то конкретной особенности объекта: размер, форма, категория, вид. Внутри одного кластера объекты могут различаться по другим критериям, но хотя бы по одному они должны быть схожи. Кластеризация методом k-средних (k-means). Популярный и широко используемый алгоритм, который разбивает данные на так называемые k-кластеры. Каждая точка данных присваивается ближайшему кластерному центру, а центры кластеров пересчитываются итеративно. Часто используется для кластеризации документов, сжатия изображений и сегментации рынка

4.2.1 Ассациация, Apriori и Eclat 
Ассоциация в обучении без учителя — это методика, при которой проверяется наличие зависимости одного элемента данных от другого и ведётся поиск интересных отношений или ассоциативных связей между переменными в наборе данных. Общие методы ассоциации включают алгоритмы Apriori и Eclat.
Один из самых известных алгоритмов для этого — Apriori. Он помогает находить частые наборы элементов и генерировать ассоциативные правила. Например, в магазине игрушек рядом с Лего можно найти кубики 
Пример использования: в магазине игрушек можно заметить, что Лего и кубики благодаря тому ,что стоят рядом и часто покупают в паре, то есть благодаря нейросети можно разделять продукты на пары тем самым составить ассортимент продуктов на полке таким образом, что бы сосед был в паре хорошо покупаем и для идеального результата, что бы люди обращали внимания один из товаров продавать по скидке.

4.2.2 Сокращение размерности , метод главных компонентов 
Метод главных компонент (PCA -  Principal Component Analysis) — один из самых интуитивно простых и часто используемых методов для снижения размерности данных и проекции их на ортогональное подпространство признаков.
В совсем общем виде это можно представить как предположение о том, что все наши наблюдения скорее всего выглядят как некий эллипсоид в подпространстве нашего исходного пространства и наш новый базис в этом пространстве совпадает с осями этого эллипсоида. Это предположение позволяет нам одновременно избавиться от сильно скоррелированных признаков, так как вектора базиса пространства, на которое мы проецируем, будут ортогональными.
 
Рис 4.2

В общем случае размерность этого эллипсоида будет равна размерности исходного пространства, но наше предположение о том, что данные лежат в подпространстве меньшей размерности, позволяет нам отбросить "лишнее" подпространство в новой проекции, а именно то подпространство, вдоль осей которого эллипсоид будет наименее растянут. Мы будем это делать "жадно", выбирая по очереди в качестве нового элемента базиса нашего нового подпространства последовательно ось эллипсоида из оставшихся, вдоль которой дисперсия будет максимальной.




 
ГЛАВА 5 NLP

В этой главе, мы наконец то рассмотрим две главные системы: рекуррентные нейронные сети (RNN) и сверточные нейронные сети (CNN), мы рассмотрим их с кодом, так что разобравшись вы можете повторить код и на практике посмотреть  работу нейросети, так же начиная с этой главы, последующие будут заканчиваться заданиями для практики и более глубокого понимания прочитанного. Обработка естественного языка (NLP) — это большая сфера, где нейронные сети достигли хороших результатов. В NLP применяются разные типы нейросетей, которые помогают анализировать текст. Давайте посмотрим на две главные системы.
А и еще, давайте договоримся, что когда я буду использовать различные схемы, я разные объекты буду помечать условными знаками, их значение вы можете видеть ниже
 
Рис 4.1


5.1 RNN

RNN — это тип нейронной сети, которая специализируется на обработке последовательных данных. В отличие от стандартных нейронных сетей, RNN имеет “память” благодаря петле внутри своих слоев(рис 4.2, на нем можно видеть векторный перенос который отвечает за цикл), которая позволяет передавать информацию от одного шага последовательности к следующему. Эта способность делает RNN идеальными для обработки данных, где предыдущие значения влияют на последующие, таких как текст, временные ряды или аудиосигналы.RNN так же деляться
 
рис 4.2


5.1.1  LSTM

LSTM — это особый тип RNN, разработанный для решения проблемы долгосрочных зависимостей, с которыми сталкиваются обычные RNN. LSTM имеет более сложную структуру, включая специальные механизмы, называемые “ячейками памяти” и “воротами” (gates), которые помогают контролировать поток информации.
Любая рекуррентная нейронная сеть имеет форму цепочки повторяющихся модулей нейронной сети. В обычной RNN структура одного такого модуля очень проста, например, он может представлять собой один слой с функцией активации
 
Рис 4.3 
Если что, tanh – это тип функции активации, часто используемый в нейронных сетях. Она принимает вещественное число и преобразует его в диапазон от -1 до 1. Если рисунок 4.3 вам не понятен или хотите углубиться в эту тему, совету к прочтению статью https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami (ссылка содержит информацию как по LSTM, так и  GRU)
Теперь, давайте посмотрим как реализовать архитектуру в коде (если что в таком случае задача(метод) – класификация)
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split
# Создадим синтетический датасет для иллюстрации
texts = ["Сегодня хорошая погода.","Завтра будет солнечно.","Лето — лучшее время года.","Дождь идет весь день."]
labels = [1, 1, 2, 0] # 0 — дождь, 1 — солнце, 2 — лето
# Токенизация и векторизация текстов (в данном случае, просто индексирование)
tokenizer = tf.keras.layers.TextVectorization()
tokenizer.adapt(texts)
# Создание модели LSTM
model = Sequential()
model.add(tokenizer)
model.add(Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=16, input_length=6))
model.add(LSTM(16))
model.add(Dense(3, activation="softmax")) # Три класса: дождь, солнце, лето
# Компиляция модели
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
# Создание фиктивных данных для обучения и теста
x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)
# Обучение модели
model.fit(x_train, y_train, epochs=10, verbose=0) # Модель будет идеально подстраиваться под эти простые данные
# Оценка модели
accuracy = model.evaluate(x_test, y_test)[1] # Извлекаем точность из метрик
print(f"Точность: {accuracy:.4f}")
В данном коде мы имеем простой синтетический датасет с четырьмя текстами, каждому из которых присвоена уникальная метка. Модель LSTM будет идеально обучена для этого набора данных и даст точность 1.0. Однако в реальных задачах точность обычно ниже из-за сложности данных и пересечений между классами.





5.1.2 GRU

GRU — это еще один тип RNN(не Грю из Гадкого я, не путайте), который также предназначен для борьбы с проблемами долгосрочных зависимостей, является аналогом LSTM но с более простой структурой. GRU объединяет некоторые из функций ворот LSTM, уменьшая количество параметров и вычислительных затрат. Все потому, что в GRU нету отдельной ячейки памяти, вместо этого оно обновляеться скрытое состояние на прямую

 


import numpy as np

import pandas as pd

from sklearn.preprocessing import MinMaxScaler

from tensorflow.keras.models import Sequential


from tensorflow.keras.layers import GRU, Dense

from tensorflow.keras.optimizers import Adam

# Создание модели GRU 
model = Sequential()

model.add(GRU(units=50,return_sequences=True,input_shape=(X.shape,1))) #Добавляет слой с 50 нейронами

model.add(GRU(units=50))

model.add(Dense(units=1))#выходной слой 
model.compile(optimizer=Adam(learning_rate=0.001),вloss='mean_squared_error')

model.fit(X, y, epochs=10, batch_size=32) #Поиск признаков в тексте
#Оценка модели
accuracy = model.evaluate(x_test, y_test) # Извлекаем точность из метрик
print(f"Точность: {accuracy:.4f}")

5.1.3 Tree Lstm 
Не хочу как то выделять этот способ, просто знайте, что он есть , но если вы все же интересуетесь советую погуглить и прочитать информацию на этом сайте : https://pythonhint.com/post/9816020194892458/tree-lstm-in-keras

5.2 Transformers
Что первое вы представили в голове ? Бамбелби? Ну если хоть, что то представили, значит хотя бы внимательно читаете. А вообще transformers  - это такой вид нейросетевой архитектуры, который хорошо подходит для обработки последовательностей данных. Пожалуй, самый популярный пример таких данных это предложение, которое можно считать упорядоченным набором слов.
Transformers в NLP состоят из нескольких ключевых компонентов: 
 1. Encoder и Decoder: Архитектура трансформеров включает два основных блока — энкодер, который обрабатывает входные данные, и декодер, который генерирует выходные данные. Для моделей, которые только предсказывают текст (например, BERT), используется только энкодер.  
2. Self-Attention: Это механизм, который позволяет модели обращать внимание на разные слова в предложении, учитывая их взаимосвязь. Он работает за счет вычисления весов для каждого слова относительно других.  
3. Positional Encoding: Поскольку трансформеры не используют рекуррентные слои, то для учёта порядка слов в последовательности вводится кодирование позиции, чтобы модель могла понимать их порядок.  
4. Feed-Forward Neural Networks: После механизма self-attention данные проходят через полностью связанные нейронные сети. 
5. Residual Connections и Layer Normalization: Эти механизмы помогают в обучении, позволяя передавать градиенты и ускоряя сходимость модели.
Так же приведу пример кода, но сразу говорю, выглядит он страшнее чем на деле:
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.head_dim = d_model // num_heads
        
        self.query_linear = nn.Linear(d_model, d_model)
        self.key_linear = nn.Linear(d_model, d_model)
        self.value_linear = nn.Linear(d_model, d_model)
        self.fc_out = nn.Linear(d_model, d_model)

    def forward(self, x):
        N, seq_length, _ = x.shape
        queries = self.query_linear(x).view(N, seq_length, self.num_heads, self.head_dim)
        keys = self.key_linear(x).view(N, seq_length, self.num_heads, self.head_dim)
        values = self.value_linear(x).view(N, seq_length, self.num_heads, self.head_dim)

        queries = queries.transpose(1, 2)
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)

        energy = torch.einsum("nhqd,nhkd->nhqk", [queries, keys])
        attention = F.softmax(energy / (self.d_model ** (1 / 2)), dim=3)

        out = torch.einsum("nhql,nhld->nhqd", [attention, values]).reshape(N, seq_length, self.d_model)
        return self.fc_out(out)

class FeedForward(nn.Module):
    def __init__(self, d_model, ff_hidden):
        super(FeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, ff_hidden)
        self.fc2 = nn.Linear(ff_hidden, d_model)

    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, ff_hidden):
        super(TransformerBlock, self).__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ff = FeedForward(d_model, ff_hidden)

    def forward(self, x):
        attention = self.attention(x)
        x = self.norm1(attention + x)  # Residual connection
        forward = self.ff(x)
        x = self.norm2(forward + x)    # Residual connection
        return x

class Transformer(nn.Module):
    def __init__(self, d_model, num_heads, ff_hidden, num_layers):
        super(Transformer, self).__init__()
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, num_heads, ff_hidden) for _ in range(num_layers)
        ])

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# Пример использования
d_model = 512
num_heads = 8
ff_hidden = 2048
num_layers = 6
transformer = Transformer(d_model, num_heads, ff_hidden, num_layers)

# Создаем случайный ввод с размерами (batch_size, seq_length, d_model)
input_tensor = torch.rand(10, 20, d_model)
output = transformer(input_tensor)
print(output.shape)  # Должно быть (10, 20, 512)
Этот код создает простую архитектуру 
Главное преимущество трансформеров заключается в их способности обрабатывать длительные зависимости в последовательностях. Кроме того, они очень производительны, могут обрабатывать последовательности параллельно. Это особенно полезно в задачах вроде машинного перевода, анализа настроений и синтеза текста. 
5.3 Машинное обучение
Мы до этого уже разобрали работу машинного обучения, поэтому в сфере NLP предлагаю рассмотреть сразу на примере, пример кода и задачу к данному разделу можете найти в 6 главе про практику.


 
ГЛАВА 6 Практические задачи к главам
Задания к главам вы найдете в репозитории по ссылке - , что бы не заскучать я построюсь на протяжении двух лет каждые два месяца добавлять различные задачи, все так же по теме глав. 
ГЛАВА 7 ИТОГ
До подведения итогов хочу еще выделить ваше время на информацию которая еще не упоминаласб ранее, но я считаю важным дополнением
7.1 NLP не один такой?
Вот сидите вы и читаете  итоговую главу, до прочтения заголовка данной подглавы вы думали , что NLP один в своем роде для обработки речи и захватывает все задачи которые становятся перед людьми (в плане задач для языковых моделей ),однако, в этой сфере есть еще три страшные буквы  - LLM. Давайте сразу начну с различий, потому, что их теоритическое описание не сильно отличается. 
1)	NLP – содержит множество методов для решения различных типов задач, соответсвенно для каждой задачи свой небольшой(если сравнивать с датасетами LLM) датасет. LLM в свою очередь же используется для понимания текста на большом количестве данных (соответственно обучается решать сразу множество задач)
2)	NLP  может использует довольно простые методы (линейная регрессии и другие методы машинного обучения). LLM создаётся только из слоев (то есть только глубоким обучением)
3)	Еще хочу выделить доступность, так как вам не совсем понятно почему я рассказываю об LLM в конце, ведь можно было бы выделить отдельную главу, так же разобрать создание моделей и т.д, однако, есть одна проблемы – доступность. NLP – широко доступен благодаря множеству доступных инструментов и библиотек, а так же не требует огромных датасетов  , в свою очередь LLM очень требовательный к вычислительным мощностям ( как мы уже говорили нужен большой датасет, а следовательно боьшое количество свободной памяти  на устройстве и так же хороший вычеслительный проццессор для переработке вс этого датасета ) 
Так же хочу отметить LLMв старую эпоху – это были так называемые N-граммные языковые модели, если вам интересно окунуться в старую эпоху то советую прочитать https://habr.com/ru/companies/ntr/articles/809701/
7.2 Интерфейсы
Если после прочтения вы решите сами для себя или для проекта создать языковую нейрсоеть, то получитее сырьевой проект, для полного проеображения советую добавить интерфейс. Что бы вам было легче ниже я выпишу статьи которые помогут вам обучится писать телеграмм ботов (aiogram – название библиотеки, для написания тг ботов) и консольных приложений (на tkinеer – так же библиотека, только для написаний консольных приложений)
Tkinter - https://habr.com/ru/articles/133337/
https://metanit.com/python/tkinter/
aoigram - https://habr.com/ru/companies/amvera/articles/820527/
P.S у aoigram есть аналоги, так что выбирайте, что удобнее и подходит именно вам(https://telegra.ph/V-chem-raznica-mezhdu-Aiogram-i-TeleBot-V-chem-otlichiya-mezhdu-Aiogram-i-TeleBot-plyusy-i-minusy-kazhdoj-biblioteki-03-20   - есть плюсы и минусы)
Если вы все же выберете telebot то могу посоветовать https://habr.com/ru/articles/580408/ 
7.3 Что дальше?
После прочтения вы не знаете куда резвится еще дальше? Тогда продолжайте читать данную под главу. Ответ очень прост, но требует не мало фантазии – придумайте свой проект. Когда вы придумаете идею которую решит языковая модель, вы начнете писать код и сталкиватся с идеями других, слушать их советы и узнавать еще больше в NLP.

7.4 Результаты
Надеюсь после прочтения, знания в области NLP укрепились и перестали казаться магией, зато теперь для ваших знакомых не читавших данное пособие вы стали магом. Я так же надеюсь, что вы поделитесь полученными знаниями с другими или захотите дополнить данное пособие, если же второе то пожалуйста свяжитесь с автором по почте masasaposnik24@gmail.com .
Желаю читавшему удачи и дальнейшего развития !

 
Список литературы:
https://habr.com/ru/articles/509472/ 
https://skillbox.ru/media/code/nlp-chto-eto-takoe-i-kak-ona-rabotaet/ 
https://kartaslov.ru/книги/Джейд_Картер_Нейросети_Обработка_естественного_языка/3 
https://habr.com/ru/companies/oleg-bunin/articles/352614/ 
https://docviewer.yandex.ru/view/1767593172/?page=29&*=Ul3eA5gnl9HZp7I%2BVQ1XH2Q0d%2B57InVybCI6Imh0dHBzOi8vd3d3LmdvcmJhY2hldi5hbS9maWxlcy9saWJyYXJ5L25scC9OTFAlMjBUdXRvcmlhbC5wZGYiLCJ0aXRsZSI6Ik5MUCBUdXRvcmlhbC5wZGYiLCJub2lmcmFtZSI6dHJ1ZSwidWlkIjoiMTc2NzU5MzE3MiIsInRzIjoxNzQxMTc5NTQ2NzU5LCJ5dSI6IjYyNjA4NTIxMDE2OTM0ODQ0MjUiLCJzZXJwUGFyYW1zIjoidG09MTc0MTE3OTE1NiZ0bGQ9cnUmbGFuZz1ydSZuYW1lPU5MUCUyMFR1dG9yaWFsLnBkZiZ0ZXh0PUxOUCslRDElOEYlRDAlQjclRDElOEIlRDAlQkElRDAlQkUlRDAlQjIlRDElOEIlRDAlQjUrJUQwJUJEJUQwJUI1JUQwJUI5JUQxJTgwJUQwJUJFJUQxJTgxJUQwJUI1JUQxJTgyJUQwJUI4KyVEMCVCRiVEMCVCRSVEMSU4MSVEMCVCRSVEMCVCMSVEMCVCOCVEMCVCNSZ1cmw9aHR0cHMlM0EvL3d3dy5nb3JiYWNoZXYuYW0vZmlsZXMvbGlicmFyeS9ubHAvTkxQJTI1MjBUdXRvcmlhbC5wZGYmbHI9NTUmbWltZT1wZGYmbDEwbj1ydSZzaWduPWFiNTFlMWYxOGJiZWNkNTVjOWU0MmExN2FlMDhhOGI1JmtleW5vPTAifQ%3D%3D&lang=ru 
https://uzundemir.github.io/RNN 
https://habr.com/ru/companies/mvideo/articles/780774/ 
https://dzen.ru/a/Yj8hzGjxi2CFLVcN 
https://www.scaler.com/topics/deep-learning/gru-network/ 
https://habr.com/ru/companies/otus/articles/886816/ 
https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami 
https://habr.com/ru/companies/ntr/articles/809701/ 
https://habr.com/ru/companies/Voximplant/articles/446738/ 
https://skillbox.ru/media/code/nlp-chto-eto-takoe-i-kak-ona-rabotaet/ 
https://habr.com/ru/companies/mws/articles/770202/ 
https://habr.com/ru/articles/133337/ 
 https://iq.opengenus.org/lemmatization-in-nlp/ 
https://zilliz.com/learn/transforming-text-the-rise-of-sentence-transformers-in-nlp 
 


